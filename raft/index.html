<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Raft Consensus</title>
  <link rel="stylesheet" href="css/reveal.css">
  <link rel="stylesheet" href="css/theme/sky.css" id="theme">
  <!--Add support for earlier versions of Internet Explorer -->
  <!--[if lt IE 9]>
  <script src="lib/js/html5shiv.js"></script>
  <![endif]-->
</head>

<body>
  <!-- Wrap the entire slide show in a div using the "reveal" class. -->
  <div class="reveal">
    <!-- Wrap all slides in a single "slides" class -->
    <div class="slides">

      <!-- ALL SLIDES GO HERE -->
      <!-- Each section element contains an individual slide -->
      <section>
        <h1>The Raft Consensus Algorithm</h1>
      </section>

      <section>

        <section>
          <h3>Why?</h3>
        </section>

        <section>
          <h3>You need to a state machine with the following properties:</h3>
          <ul class="fragment">
            <li><strong>Replicated</strong> - avoid data loss</li>
            <li class="fragment"><strong>Highly Available </strong>- avoid downtime on node failure</li>
            <li class="fragment"><strong>Consistent </strong>- for data integrity</li>
            <li class="fragment"><strong>Dynamic </strong>- add new or remove nodes as needed</li>
            <li class="fragment"><strong>Simple </strong>- to understand, implement and manage</li>
            <li class="fragment"><strong>Performant </strong>- obviously</li>
          </ul>
        </section>

        <section>
          <h3>Think of things like:</h3>
          <ul class="fragment">
            <li><strong>Audit logging</strong> - durable and secure from host compromise</li>
            <li class="fragment"><strong>Event and command sourcing</strong> - persistent record of state changes</li>
            <li class="fragment"><strong>Published Serialization point</strong> - once published order is guaranteed</li>
            <li class="fragment"><strong>Consistent distributed systems</strong> - run multiple instances in parallel for seamless failover</li>
          </ul>
        </section>

      </section>

      <section>

        <section>
          <h3>History</h3>
          <ul class="fragment">
            <li>Work in this area began in the mid 1980's</li>
            <li class="fragment"><strong>Oki and Liskov</strong> published "Viewstamped Replication" in 1988</li>
            <li class="fragment">It was quickly followed by the work of <strong>Leslie Lamport</strong> who derived a family of consensus algorithms named Paxos, after a fictitious legislative consensus system used on the Greek island of Paxos</li>
            <li class="fragment"><strong>Paxos</strong> had an elegant formalism but was relatively complex and difficult to grasp</li>
            <li class="fragment">In 2013, <strong>Ongaro and Ousterhout</strong> published the Raft protocol, which aimed to be much simpler.</li>
          </ul>
        </section>

        <section>
          <h3>Raft Overview</h3>
          <ul class="fragment">
            <li>Think of it as a "replicated state machine" cluster</li>
            <li class="fragment">Client(s) publish <strong>append only command logs</strong> to the cluster</li>
            <li class="fragment">One node is considered the <strong>leader</strong>, while all others are <strong>followers</strong></li>
            <li class="fragment">The leader has <strong>complete responsibility</strong> for managing the replicated log</li>
            <li class="fragment">The leader also determines when the log should be considered <strong>committed</strong> - updating its own and followers' state machines</li>
            <li class="fragment">Clients <strong>only talk</strong> to the leader</li>
          </ul>
        </section>

        <section>
          <h3>Replicated State Machine</h3>
          <img id="rsm" src="img/rsm.png"></img>
        </section>

        <section>
          <h4>Note that Raft does not solve the <strong>Byzantine Generals</strong> problem. It requires the co-operation of all nodes to function</h4>
        </section>

        <section>
          <h3>Raft Subproblems</h3>
          <ul class="fragment">
            <li><strong>Leader election</strong> - a new leader must be chosen when the existing one fails</li>
            <li class="fragment"><strong>Log replication</strong> - leader accepts log entries and replicates them across the cluster</li>
            <li class="fragment"><strong>Safety</strong> - if any node has committed a particular log entry then no other node may apply a different command for the same log index</li>
          </ul>
        </section>

      </section> 

      <section>
        <section>
          <h3>Raft Cluster</h3>
          <ul class="fragment">
            <li>Requires <strong>majority of nodes</strong> to be functioning</li>
            <li class="fragment">For example, in a 5 node cluster, 2 nodes can fail and the system can still function</li>
            <li class="fragment">Raft supports adding or reducing nodes dynamically, allowing maintenance and load sizing as needed</li>
          </ul>
        </section>

        <section>
          <h3>Raft Node</h3>
          <ul class="fragment">
            <li>A node can be in 3 states: <strong>"leader", "follower", or "candidate"</strong></li>
            <li class="fragment">Followers are <strong>passive</strong> and only respond to requests from leaders or candidates</li>
            <li class="fragment">If a client attempts to contact a follower it <strong>redirects</strong> them to the leader</li>
            <li class="fragment">Raft assumes that leader <strong>failure</strong> is relatively rare, as the system only <strong>makes progress</strong> under stable leadership</li>
          </ul>
        </section>

        <section>
          <i>It is legal because I wish it</i> - Louie XIV
        </section>

        <section>
          <h3>Leader Terms</h3>
          <ul class="fragment">
            <li>Raft divides time into <strong>terms</strong> of arbitrary length</li>
            <li class="fragment">Terms are numbered and are <strong>continuous and strictly monotonic</strong></li>
            <li class="fragment">Each term begins with an <strong>election</strong> in which one or more candidate nodes vie to become leader</li>
            <li class="fragment">In the case of a split vote, that term ends immediately with no winners and a <strong>new election</strong> begins for another term</li>
          </ul>
        </section>

        <section>
          <h3>Leader Terms</h3>
          <ul class="fragment">
            <li>Terms serve as a logical clock for the protocol and allow nodes to <strong>detect outdated</strong> information</li>
            <li class="fragment">Each node stores its own view of the current term and whenever nodes communicate they <strong>exchange term numbers</strong></li>
            <li class="fragment">If a node receives a term number that is greater than its own it <strong>updates its term</strong> value to match</li>
          </ul>
        </section>

        <section>
          <h3>Terms</h3>
          <img id="terms" src="img/terms.png"></img>
        </section>
      </section>

      <section>
        <section>
          <h3>Node Communication Protocol</h3>
          <ul class="fragment">
            <li>There are only 2 RPCs in Raft: AppendLog and RequestVote</li>
            <li class="fragment"><strong>AppendLogs</strong> are issued by the leader to each follower to ask them to update their logs with the attached log list.</li>
            <li class="fragment">AppendLog is also used with an empty list as a <strong>heartbeat</strong></li>
            <li class="fragment"><strong>RequestVotes</strong> are issued by <strong>candidates</strong> to <strong>each follower</strong> when an election begings</li>
            <li class="fragment">RPCs are issued in <strong>parallel</strong> and are <strong>retried indefinately</strong> if a response is not received in a timely manner</li>
          </ul>
        </section>
      </section>

      <section>
        <section>
          <h3>Leader Election</h3>
          <ul class="fragment">
            <li>All nodes begin life as followers</li>
            <li class="fragment">They remain followers as long as they <strong>receive valid RPCs from leaders or candidates</strong></li>
            <li class="fragment">If a follower doesn't hear from others for a period of time (the <strong>election timeout</strong>, they assume the leader is dead</li>
            <li class="fragment">The follower increments its term number, <strong>switches to candidate state</strong>, votes for itself and issues RequestVote RPCs to all other nodes</li>
            <li class="fragment">A candidate remains in this state until it either wins the election, another node establishes itself as leader, or a period of time goes by with no winner</li>
          </ul>
        </section>

        <section>
          <h3>Winning the Election</h3>
          <ul class="fragment">
            <li>When a node receives a RequestVote it votes on a <strong>first come, first serve</strong> basis for that candidate for that particular term</li>
            <li>If a candidate receives a majority of votes it <strong>considers itself leader</strong> and issues heartbeats to <strong>establish its authority</strong> with the other nodes</li>
            <li class="fragment">By allowing only one vote per term and requiring majority rule, the system guarantees that their is <strong>only one leader at a time</strong></li>
          </ul>
        </section>

        <section>
          <i>A multitude of rulers is not a good thing. Let there be one ruler, one king.</i> - Homer
        </section>

        <section>
          <h3>Losing the Election</h3>
          <ul class="fragment">
            <li>While a candidate node is waiting for votes it can receive an <strong>AppendLog</strong> RPC from another node</li>
            <li class="fragment">If the term number of the other node is at <strong>least as large as the candidate's term</strong> then it goes back to a follower state</li>
            <li class="fragment">If the term number is smaller the candidate <strong>rejects that leader's purported authority</strong> and continues waiting for votes</li>
          </ul>
        </section>

        <section>
          <h3>Split Votes</h3>
          <ul class="fragment">
            <li>It's possible for there to be a split vote among candidates where <strong>no one wins</strong></li>
            <li class="fragment">After a certain time, candidates give up and <strong>restart another election</strong></li>
            <li class="fragment">The timeout is <strong>randomly selected</strong> to minimize the chance of repeated election cycles</li>
          </ul>
        </section>

        <section>
          hanging chad
        </section>

        <section>
          <h3>Node Transitions</h3>
          <img id="transitions" src="img/transitions.png"></img>
        </section>
      </section>

      <section>
        <p>This is the basics of leader elections.  Raft imposes additional constraints on who can win an election that we will discuss later</p>
      </section>

      <section>
        <section>
          <h3>Log replication</h3>
          <ul class="fragment">
            <li>The leader receives client requests with contain a <strong>command that will be executed</strong> upon the state machine</li>
            <li class="fragment">The leader appends the command to <strong>its log</strong> and then issues AppendLog RPCs in <strong>parallel</strong> to the other nodes</li>
            <li class="fragment">Once a <strong>majority of the nodes responds</strong> the leader commits the entry and <strong>executes the command</strong>, returning the result to the client</li>
          </ul>
        </section>

        <section>
          <h3>Log Replication</h3>
          <ul clas="fragment">
            <li>The leader continually <strong>retries AppendLogs</strong> against nodes that have failed to respond, even <strong>after it has replied</strong> to the client</li>
            <li class="fragment">Followers <strong>learn of committed entries</strong> because the leader includes the highest <strong>index commited</strong> in future AppendLog RPCs</li>
            <li class="fragment">Once followers commit a log they too <strong>execute the command</strong> upon their own internal state machines</li>
          </ul>
        </section>

        <section>
          <h3>Logs</h3>
          <img id="logs" src="img/logs.png"></img>
        </section>

        <section>
          <h3>Log Matching Properties</h3>
          <p><strong>PROPOSITION 1: </strong>If two entries in different logs have the same index and term then they <strong>store the same command</strong></p>
        </section>

        <section>
          <h3>Log Matching Properties</h3>
          <ul class="fragment">
            <li>This follows from the fact that each leader is guaranteed to create at most <strong>one log entry with a given log index</strong> for a given term</li>
            <li class="fragment">Also logs are <strong>never reordered</strong></li>
          </ul>
        </section>

        <section>
          <h3>Log Matching Properties</h3>
          <p><strong>PROPOSITION 2: </strong>If two entries in different logs have the same index and term the the logs are identical in <strong>all preceeding entries</strong></p>
        </section>

        <section>
          <h3>Log Matching Properties</h3>
          <ul class="fragment">
            <li>This is guaranteed by a <strong>consistency check</strong> in AppendLog.</li>
            <li class="fragment">Leaders include the <strong>previous term and log index</strong> of the entry that <strong>preceeds</strong> the new entries.</li>
            <li class="fragment">Followers will only apply logs if these <strong>match</strong> its previous as well</li>
            <li class="fragment">Thus once a follower returns success to an AppendLog the leader knows that the <strong>follower's log is identical</strong> to its own up through the new entries</li>
          </ul>
        </section>

        <section>
          <h3>Dealing With Inconsistencies</h3>
          <ul class="fragment">
            <li>If leaders crash routinely then it is possible for an <strong>inconsistent state</strong> to occur when a new leader comes to power</li>
            <li class="fragment">Followers can be <strong>missing</strong> logs that the leader has, have <strong>additional</strong> logs that leader doesn't have or <strong>both</strong></li>
            <li class="fragment">Raft solves this by <strong>forcing</strong> all the followers' logs to be <strong>overriden</strong> by the leader's logs</li>
            <li class="fragment">While by itself this would not guarantee consistent commit order across the nodes, by adding <strong>additional restrictions</strong> on who can become elected leader this will in fact work</li>
          <ul>
        </section>

        <section>
        </section>

        <section>
          <h3>Overwriting Logs</h3>
          <ul>
            <li class="fragment">For each follower the leader maintains a <strong>nextIndex</strong> property that indicates which log index will be sent next to that <strong>particular follower</strong></li>
            <li class="fragment">When a leader comes into power it sets the last index for all followers to be the one <strong>just after the last one</strong> in its own log</li>
            <li class="fragment">When the AppendLog RPC is <strong>rejected by a follower</strong> because it violates the <strong>Log Matching Principle</strong>, the leader decrements the nextIndex and retries</li>
            <li class="fragment">Eventually the Log Matching Principle will be <strong>satisfied</strong> and the follower's logs are <strong>truncated and then extended</strong> with the leader's</li>
          </ul>
        </section>

        <section>
          <i>"Off with their heads!"</i> - The Queen of Hearts
        </section>
      </section>

      <section>
        <p>We have seen the basics of Raft.  In the next section we'll see some wrinkles</p>
      </section>

      <section>

        <section>
          <h3>Safety</h3>
          <ul class="fragment">
          </ul>
        </section>

        <section>
          <h3>Election Restriction</h3>
          <ul class="fragment">
            <li>Raft only allows candidates who contain all the i<strong>committed</strong> entries for <strong>previous terms</strong> to become leaders</li>
            <li class="fragment">This is enforced by having candidates provide their latest term and index to voters in the <strong>RequestVote</strong> RPC</li>
            <li class="fragment">Voters will <strong>reject candidates</strong> whose term is either <strong>less</strong> than theirs or who have a <strong>shorter log chain</strong> for the current term</li>
            <li class="fragment">This ensures only candidates with all committed entries can become Il Duce</li>
          </ul>
        </section>
      </section>

      <section>
      </section>


      <section>
        <h2>Questions?</h2>
      </section>

      <section>
        <h3>Links</h3>
      </section>


      </div>
  </div>

  <style>
    img {
      max-height: 550px !important;
    }

    #rsm {
      height: 500px;
    }

//    ul {
//      list-style: none !important;
//    }

//    ul li:before {
//      content: "\00BB \0020";
//    }
  </style>


  <script src="lib/js/head.min.js"></script>
  <script src="js/reveal.js"></script>
  <script src="//code.jquery.com/jquery-1.11.2.min.js"></script>

  <script>
    // Required, even if empty.
    Reveal.initialize({
    });
  </script>
</body>
</html>
